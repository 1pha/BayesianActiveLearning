{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code\")\n",
    "from dataset import build_dataset, build_dataloader\n",
    "from config import DataArguments, TrainerArguments, ModelArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11/24/2021 13:45:48] INFO - dataset: Initialize Train Dataset.\n",
      "[11/24/2021 13:45:48] INFO - dataset: Remove abstract.\n",
      "[11/24/2021 13:45:48] INFO - dataset: Using cached dataset, wasn't able to remove columns.\n",
      "[11/24/2021 13:45:48] INFO - dataset: Use the full dataset, for train dataset of total 47250 papers.\n",
      "[11/24/2021 13:45:48] INFO - dataset: Train dataset was successfully initialized.\n",
      "[11/24/2021 13:45:48] INFO - dataset: Successfully loaded mapper file ..\\assets\\area2idx.json\n",
      "[11/24/2021 13:45:48] INFO - dataset: Use tokenized_paperswithcode dataset.\n",
      "[11/24/2021 13:45:48] INFO - dataset: Preprocessed dataset. Use default vocab_size=83931.\n",
      "[11/24/2021 13:45:48] INFO - dataset: Successfully converted dataset to dataloader.\n"
     ]
    }
   ],
   "source": [
    "data_args = DataArguments()\n",
    "training_args = TrainerArguments()\n",
    "model_args = ModelArguments()\n",
    "\n",
    "data_args.data_dir = \"../data/\"\n",
    "data_args.asset_dir = \"../assets/\"\n",
    "model_args.asset_dir = data_args.asset_dir\n",
    "data_args.seed = training_args.seed\n",
    "data_args.max_seq_len = model_args.max_seq_len\n",
    "\n",
    "data_args.init_pct = 1\n",
    "training_args.do_train = True\n",
    "training_args.use_gpu = False\n",
    "\n",
    "train_dataset, model_args.vocab_size, model_args.num_labels = build_dataset(data_args, \"train\")\n",
    "train_dataloader = build_dataloader(train_dataset, data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11/24/2021 13:45:49] INFO - trainer: Bert was selected. Start Transformer setup.\n",
      "[11/24/2021 13:45:49] INFO - trainer: Successfully setup transformer settings.\n"
     ]
    }
   ],
   "source": [
    "from trainer import NaiveTrainer\n",
    "trainer= NaiveTrainer(\n",
    "    data_args,\n",
    "    training_args,\n",
    "    model_args,\n",
    "    training_dataset=train_dataloader if training_args.do_train else None,\n",
    "    validation_dataset=valid_dataloader if training_args.do_valid else None,\n",
    "    test_dataset=test_dataloader if training_args.do_test else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "loss_fc = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit, prediction = trainer.model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 1, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.one_hot(batch[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fc(logit, torch.nn.functional.one_hot(batch[\"labels\"]).type(torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  7, 13, 10,  7,  7,  7, 10,  2, 15,  7, 14,  7, 15, 10, 10,  7,  4,\n",
       "        14, 14,  8,  7,  3, 14, 12, 10,  8, 12, 10,  7, 15,  7,  9,  7, 14,  3,\n",
       "        12, 15,  4, 15, 14,  7, 10,  7,  7,  7,  4,  5,  4, 12,  3,  7, 15,  3,\n",
       "        14, 12, 12, 10,  7, 12,  3,  3, 15,  7,  8, 15,  3,  7,  2, 14,  3,  7,\n",
       "         3, 10, 12,  3,  7, 15, 14, 15, 13,  7,  4, 12,  7,  7,  4, 14,  3, 10,\n",
       "         8,  0, 15,  7,  3,  7,  8,  7, 15,  7, 15, 14, 15, 15,  4,  3, 14, 12,\n",
       "        15,  3,  1,  5,  3,  3, 10,  4, 10, 10,  7,  6, 10,  7,  3,  5,  6, 10,\n",
       "        10, 11])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0227, -0.1007,  0.0594,  ..., -0.1450,  0.2345,  0.0408],\n",
       "        [-0.0349, -0.0310,  0.0113,  ...,  0.0155,  0.3134,  0.0341],\n",
       "        [ 0.1405, -0.1610, -0.0071,  ..., -0.1379,  0.2116,  0.1854],\n",
       "        ...,\n",
       "        [ 0.1185, -0.0879,  0.0674,  ..., -0.0012,  0.2268,  0.0956],\n",
       "        [ 0.0611, -0.0226,  0.1324,  ..., -0.0433,  0.2603,  0.1079],\n",
       "        [ 0.0659, -0.1044,  0.0750,  ...,  0.0548,  0.2539, -0.0188]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "* X=[1000, 1000, 1000]\n",
      "* j=0\n",
      "\n",
      "logsumpexp([1000, 1000, 1000]): 1001.0986122886682\n",
      "logsumpexp([1000, 1000, 1000]): bombed! (naive)\n",
      "log(softmax(0, [1000, 1000, 1000]) = -1.0986122886681642 --> softmax = 0.33333333333331516\n",
      "****************************** \n",
      "\n",
      "******************************\n",
      "* X=[-1000, -1000, -1000]\n",
      "* j=0\n",
      "\n",
      "logsumpexp([-1000, -1000, -1000]): -998.9013877113318\n",
      "logsumpexp([-1000, -1000, -1000]): bombed! (naive)\n",
      "log(softmax(0, [-1000, -1000, -1000]) = -1.0986122886681642 --> softmax = 0.33333333333331516\n",
      "****************************** \n",
      "\n",
      "******************************\n",
      "* X=[1, 1, 1]\n",
      "* j=0\n",
      "\n",
      "logsumpexp([1, 1, 1]): 2.09861228866811\n",
      "logsumpexp([1, 1, 1]): 2.0986122886681096 (naive)\n",
      "log(softmax(0, [1, 1, 1]) = -1.09861228866811 --> softmax = 0.33333333333333326\n",
      "log(softmax(0, [1, 1, 1], naive) = -1.0986122886681096\n",
      "****************************** \n",
      "\n",
      "******************************\n",
      "* X=[1000, 1, 2, 3]\n",
      "* j=0\n",
      "\n",
      "logsumpexp([1000, 1, 2, 3]): 1000.0\n",
      "logsumpexp([1000, 1, 2, 3]): bombed! (naive)\n",
      "log(softmax(0, [1000, 1, 2, 3]) = 0.0 --> softmax = 1.0\n",
      "****************************** \n",
      "\n",
      "******************************\n",
      "* X=[-1000, 1, 2, 3]\n",
      "* j=0\n",
      "\n",
      "logsumpexp([-1000, 1, 2, 3]): 3.4076059644443806\n",
      "logsumpexp([-1000, 1, 2, 3]): 3.40760596444438 (naive)\n",
      "log(softmax(0, [-1000, 1, 2, 3]) = -1003.4076059644444 --> softmax = 0.0\n",
      "log(softmax(0, [-1000, 1, 2, 3], naive) = -1003.4076059644444\n",
      "****************************** \n",
      "\n",
      "******************************\n",
      "* X=[1000, 1e-05, 1e-10]\n",
      "* j=0\n",
      "\n",
      "logsumpexp([1000, 1e-05, 1e-10]): 1000.0\n",
      "logsumpexp([1000, 1e-05, 1e-10]): bombed! (naive)\n",
      "log(softmax(0, [1000, 1e-05, 1e-10]) = 0.0 --> softmax = 1.0\n",
      "****************************** \n",
      "\n",
      "******************************\n",
      "* X=[1000, 1e-05, 1e-10]\n",
      "* j=1\n",
      "\n",
      "logsumpexp([1000, 1e-05, 1e-10]): 1000.0\n",
      "logsumpexp([1000, 1e-05, 1e-10]): bombed! (naive)\n",
      "log(softmax(1, [1000, 1e-05, 1e-10]) = -999.99999 --> softmax = 0.0\n",
      "****************************** \n",
      "\n",
      "******************************\n",
      "* X=[1000, 1e-05, 1e-10]\n",
      "* j=2\n",
      "\n",
      "logsumpexp([1000, 1e-05, 1e-10]): 1000.0\n",
      "logsumpexp([1000, 1e-05, 1e-10]): bombed! (naive)\n",
      "log(softmax(2, [1000, 1e-05, 1e-10]) = -999.9999999999 --> softmax = 0.0\n",
      "****************************** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This module demos the LogSumExp trick. See https://blog.feedly.com/?p=10329\n",
    "\"\"\"\n",
    "import math\n",
    "from typing import List\n",
    "import logging\n",
    "import time\n",
    "\n",
    "\n",
    "def log_sum_exp_naive(X:List[float]) -> float:\n",
    "    \"\"\"\n",
    "    a naive calculation of LogSumExp expressions\n",
    "    :param X: a list of numbers\n",
    "    :return: the LogSumExp calculation\n",
    "    \"\"\"\n",
    "    logging.debug('START lse_naive(%s)', X)\n",
    "    try:\n",
    "        summation = 0\n",
    "        for x_i in X:\n",
    "            v = math.e**x_i\n",
    "            logging.debug('e^%f = %.5f', x_i, v)\n",
    "            summation += v\n",
    "        return math.log(summation)\n",
    "    except Exception as e:\n",
    "        logging.debug('lse_naive FAILURE')\n",
    "        raise e\n",
    "\n",
    "\n",
    "def log_sum_exp(X:List[float]) -> float:\n",
    "    \"\"\"\n",
    "    a better calculation of LogSumExp expressions\n",
    "    :param X: a list of numbers\n",
    "    :return: the LogSumExp calculation\n",
    "    \"\"\"\n",
    "    logging.debug('START lse(%s)', X)\n",
    "    c = max(X)\n",
    "    summation = 0\n",
    "    for x_i in X:\n",
    "        v = math.e ** (x_i - c)\n",
    "        logging.debug('e^(%f - c) = %.5f', x_i, v)\n",
    "    summation += sum(math.e ** (x_i - c) for x_i in X)\n",
    "\n",
    "    logging.debug('c=%.5f; summation=%.5f', c, summation)\n",
    "\n",
    "    return math.log(summation) + c\n",
    "\n",
    "\n",
    "def log_softmax(j:int, X:List[float], naive:bool=False) -> float:\n",
    "    \"\"\"\n",
    "    a log softmax calculation\n",
    "    :param j: an index into X that selects the numerator value.\n",
    "    :param X: a list of numbers\n",
    "    :param naive: use the naive LogSumExp method\n",
    "    :return: the log softmax calculation\n",
    "    \"\"\"\n",
    "    lse = log_sum_exp_naive if naive else log_sum_exp\n",
    "    return X[j] - lse(X)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(level='INFO')  # change to debug to print intermediate calculations\n",
    "\n",
    "    def _run_example(j:int, X:List[float]) -> None:\n",
    "        print('*' * 30)\n",
    "        print(f'* X={X}')\n",
    "        print(f'* j={j}\\n')\n",
    "        time.sleep(0.001) # so the logs get printed out nicely\n",
    "        y1 = log_sum_exp(X)\n",
    "        try:\n",
    "            y2 = log_sum_exp_naive(X)\n",
    "            if abs(y1 - y2) > 1e-6:\n",
    "                raise ValueError(f'calculation error {y1} != {y2}')\n",
    "        except:\n",
    "            y2 = 'bombed!'\n",
    "\n",
    "        print(f'logsumpexp({X}): {y1}')\n",
    "        print(f'logsumpexp({X}): {y2} (naive)')\n",
    "\n",
    "        ls = log_softmax(j, X)\n",
    "        print(f'log(softmax({j}, {X}) = {ls} --> softmax = {math.e**ls}')\n",
    "        if isinstance(y2, float):\n",
    "            ls = log_softmax(j, X, True)\n",
    "            print(f'log(softmax({j}, {X}, naive) = {ls}')\n",
    "\n",
    "        print('*' * 30,'\\n')\n",
    "\n",
    "    # the examples from the blog post plus a small numerically stable example\n",
    "    _examples = [[1000]*3, [-1000]*3, [1,1,1]]\n",
    "\n",
    "    for _example in _examples:\n",
    "        _run_example(0, _example)\n",
    "\n",
    "    # one huge X value\n",
    "    _run_example(0, [1000, 1, 2, 3])\n",
    "\n",
    "    # one huge negative X value\n",
    "    _run_example(0, [-1000, 1, 2, 3])\n",
    "\n",
    "    # run this in debug mode to see what happens to the contributions of the values < 1 in the logsumexp calculation and\n",
    "    # also what happens to the softmax probability distribution.\n",
    "    _run_example(0, [1000, 1e-5, 1e-10])\n",
    "    _run_example(1, [1000, 1e-5, 1e-10])\n",
    "    _run_example(2, [1000, 1e-5, 1e-10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from toma import toma\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from batchbald_redux import joint_entropy\n",
    "\n",
    "K = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_mixture_prob_dist(p1, p2, m):\n",
    "    return (1.0 - m) * np.asarray(p1) + m * np.asarray(p2)\n",
    "\n",
    "\n",
    "p1 = [0.7, 0.1, 0.1, 0.1]\n",
    "p2 = [0.3, 0.3, 0.2, 0.2]\n",
    "y1_ws = [get_mixture_prob_dist(p1, p2, m) for m in np.linspace(0, 1, K)]\n",
    "\n",
    "p1 = [0.1, 0.7, 0.1, 0.1]\n",
    "p2 = [0.2, 0.3, 0.3, 0.2]\n",
    "y2_ws = [get_mixture_prob_dist(p1, p2, m) for m in np.linspace(0, 1, K)]\n",
    "\n",
    "p1 = [0.1, 0.1, 0.7, 0.1]\n",
    "p2 = [0.2, 0.2, 0.3, 0.3]\n",
    "y3_ws = [get_mixture_prob_dist(p1, p2, m) for m in np.linspace(0, 1, K)]\n",
    "\n",
    "p1 = [0.1, 0.1, 0.1, 0.7]\n",
    "p2 = [0.3, 0.2, 0.2, 0.3]\n",
    "y4_ws = [get_mixture_prob_dist(p1, p2, m) for m in np.linspace(0, 1, K)]\n",
    "\n",
    "\n",
    "def nested_to_tensor(l):\n",
    "    return torch.stack(list(map(torch.as_tensor, l)))\n",
    "\n",
    "\n",
    "ys_ws = nested_to_tensor([y1_ws, y2_ws, y3_ws, y4_ws])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7000, 0.1000, 0.1000, 0.1000],\n",
       "         [0.6789, 0.1105, 0.1053, 0.1053],\n",
       "         [0.6579, 0.1211, 0.1105, 0.1105],\n",
       "         [0.6368, 0.1316, 0.1158, 0.1158],\n",
       "         [0.6158, 0.1421, 0.1211, 0.1211],\n",
       "         [0.5947, 0.1526, 0.1263, 0.1263],\n",
       "         [0.5737, 0.1632, 0.1316, 0.1316],\n",
       "         [0.5526, 0.1737, 0.1368, 0.1368],\n",
       "         [0.5316, 0.1842, 0.1421, 0.1421],\n",
       "         [0.5105, 0.1947, 0.1474, 0.1474],\n",
       "         [0.4895, 0.2053, 0.1526, 0.1526],\n",
       "         [0.4684, 0.2158, 0.1579, 0.1579],\n",
       "         [0.4474, 0.2263, 0.1632, 0.1632],\n",
       "         [0.4263, 0.2368, 0.1684, 0.1684],\n",
       "         [0.4053, 0.2474, 0.1737, 0.1737],\n",
       "         [0.3842, 0.2579, 0.1789, 0.1789],\n",
       "         [0.3632, 0.2684, 0.1842, 0.1842],\n",
       "         [0.3421, 0.2789, 0.1895, 0.1895],\n",
       "         [0.3211, 0.2895, 0.1947, 0.1947],\n",
       "         [0.3000, 0.3000, 0.2000, 0.2000]],\n",
       "\n",
       "        [[0.1000, 0.7000, 0.1000, 0.1000],\n",
       "         [0.1053, 0.6789, 0.1105, 0.1053],\n",
       "         [0.1105, 0.6579, 0.1211, 0.1105],\n",
       "         [0.1158, 0.6368, 0.1316, 0.1158],\n",
       "         [0.1211, 0.6158, 0.1421, 0.1211],\n",
       "         [0.1263, 0.5947, 0.1526, 0.1263],\n",
       "         [0.1316, 0.5737, 0.1632, 0.1316],\n",
       "         [0.1368, 0.5526, 0.1737, 0.1368],\n",
       "         [0.1421, 0.5316, 0.1842, 0.1421],\n",
       "         [0.1474, 0.5105, 0.1947, 0.1474],\n",
       "         [0.1526, 0.4895, 0.2053, 0.1526],\n",
       "         [0.1579, 0.4684, 0.2158, 0.1579],\n",
       "         [0.1632, 0.4474, 0.2263, 0.1632],\n",
       "         [0.1684, 0.4263, 0.2368, 0.1684],\n",
       "         [0.1737, 0.4053, 0.2474, 0.1737],\n",
       "         [0.1789, 0.3842, 0.2579, 0.1789],\n",
       "         [0.1842, 0.3632, 0.2684, 0.1842],\n",
       "         [0.1895, 0.3421, 0.2789, 0.1895],\n",
       "         [0.1947, 0.3211, 0.2895, 0.1947],\n",
       "         [0.2000, 0.3000, 0.3000, 0.2000]],\n",
       "\n",
       "        [[0.1000, 0.1000, 0.7000, 0.1000],\n",
       "         [0.1053, 0.1053, 0.6789, 0.1105],\n",
       "         [0.1105, 0.1105, 0.6579, 0.1211],\n",
       "         [0.1158, 0.1158, 0.6368, 0.1316],\n",
       "         [0.1211, 0.1211, 0.6158, 0.1421],\n",
       "         [0.1263, 0.1263, 0.5947, 0.1526],\n",
       "         [0.1316, 0.1316, 0.5737, 0.1632],\n",
       "         [0.1368, 0.1368, 0.5526, 0.1737],\n",
       "         [0.1421, 0.1421, 0.5316, 0.1842],\n",
       "         [0.1474, 0.1474, 0.5105, 0.1947],\n",
       "         [0.1526, 0.1526, 0.4895, 0.2053],\n",
       "         [0.1579, 0.1579, 0.4684, 0.2158],\n",
       "         [0.1632, 0.1632, 0.4474, 0.2263],\n",
       "         [0.1684, 0.1684, 0.4263, 0.2368],\n",
       "         [0.1737, 0.1737, 0.4053, 0.2474],\n",
       "         [0.1789, 0.1789, 0.3842, 0.2579],\n",
       "         [0.1842, 0.1842, 0.3632, 0.2684],\n",
       "         [0.1895, 0.1895, 0.3421, 0.2789],\n",
       "         [0.1947, 0.1947, 0.3211, 0.2895],\n",
       "         [0.2000, 0.2000, 0.3000, 0.3000]],\n",
       "\n",
       "        [[0.1000, 0.1000, 0.1000, 0.7000],\n",
       "         [0.1105, 0.1053, 0.1053, 0.6789],\n",
       "         [0.1211, 0.1105, 0.1105, 0.6579],\n",
       "         [0.1316, 0.1158, 0.1158, 0.6368],\n",
       "         [0.1421, 0.1211, 0.1211, 0.6158],\n",
       "         [0.1526, 0.1263, 0.1263, 0.5947],\n",
       "         [0.1632, 0.1316, 0.1316, 0.5737],\n",
       "         [0.1737, 0.1368, 0.1368, 0.5526],\n",
       "         [0.1842, 0.1421, 0.1421, 0.5316],\n",
       "         [0.1947, 0.1474, 0.1474, 0.5105],\n",
       "         [0.2053, 0.1526, 0.1526, 0.4895],\n",
       "         [0.2158, 0.1579, 0.1579, 0.4684],\n",
       "         [0.2263, 0.1632, 0.1632, 0.4474],\n",
       "         [0.2368, 0.1684, 0.1684, 0.4263],\n",
       "         [0.2474, 0.1737, 0.1737, 0.4053],\n",
       "         [0.2579, 0.1789, 0.1789, 0.3842],\n",
       "         [0.2684, 0.1842, 0.1842, 0.3632],\n",
       "         [0.2789, 0.1895, 0.1895, 0.3421],\n",
       "         [0.2895, 0.1947, 0.1947, 0.3211],\n",
       "         [0.3000, 0.2000, 0.2000, 0.3000]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_2d(logits):\n",
    "\n",
    "    if logits.ndim == 3 and logits.shape[1] == 1:\n",
    "        logits = logits.squeeze()\n",
    "        return logits\n",
    "    elif logits.ndim == 3 and logits.shape[1] > 1:\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7000, 0.1000, 0.1000, 0.1000, 0.0000],\n",
       "        [0.3000, 0.3000, 0.2000, 0.2000, 0.0000]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_logits = to_2d(logits)\n",
    "_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_of_confidence(logits: np.ndarray):\n",
    "\n",
    "    logits = to_2d(logits)\n",
    "    print(logits)\n",
    "\n",
    "    part = torch.split(-logits, 1, dim=1)[0].squeeze()\n",
    "\n",
    "    print(part)\n",
    "    margin = -part[:, 0] + part[:, 1]\n",
    "    return margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margin_of_confidence_np(logits: np.ndarray):\n",
    "\n",
    "    part = np.partition(-logits, 1, axis=1)\n",
    "    print(part)\n",
    "    margin = -part[:, 0] + part[:, 1]\n",
    "    return margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "p1 = [0.7, 0.1, 0.1, 0.1, 0.0]\n",
    "p2 = [0.3, 0.3, 0.2, 0.2, 0.0]\n",
    "\n",
    "logits = torch.stack(tuple(map(torch.tensor, (p1, p2)))).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 5])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "logits_np = np.array((p1, p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.7 -0.1 -0.1 -0.1 -0. ]\n",
      " [-0.3 -0.3 -0.2 -0.2 -0. ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.6, 0. ])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "margin_of_confidence_np(logits_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7000, 0.1000, 0.1000, 0.1000, 0.0000],\n",
      "        [0.3000, 0.3000, 0.2000, 0.2000, 0.0000]])\n",
      "tensor([-0.7000, -0.3000])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22308/884549059.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmargin_of_confidence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22308/3314336005.py\u001b[0m in \u001b[0;36mmargin_of_confidence\u001b[1;34m(logits)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mmargin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mpart\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpart\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmargin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "margin_of_confidence(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = torch.split(-logits, 1, dim=1)[0].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7000, -0.1000, -0.1000, -0.1000, -0.0000],\n",
       "        [-0.3000, -0.3000, -0.2000, -0.2000, -0.0000]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1000, -0.3000])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a61f66c06004f41873e28d463485a6c1d6ba80ac66cbf9cb782a5e0aa1d62397"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('bnn': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
