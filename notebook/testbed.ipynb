{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code\")\n",
    "from dataset import build_dataset, build_dataloader\n",
    "from config import DataArguments, TrainerArguments, ModelArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11/24/2021 13:37:50] INFO - dataset: Initialize Train Dataset.\n",
      "[11/24/2021 13:37:50] INFO - dataset: Remove abstract.\n",
      "[11/24/2021 13:37:50] INFO - dataset: Using cached dataset, wasn't able to remove columns.\n",
      "[11/24/2021 13:37:50] INFO - dataset: Use the full dataset, for train dataset of total 47250 papers.\n",
      "[11/24/2021 13:37:50] INFO - dataset: Train dataset was successfully initialized.\n",
      "[11/24/2021 13:37:50] INFO - dataset: Successfully loaded mapper file ..\\assets\\area2idx.json\n",
      "[11/24/2021 13:37:50] INFO - dataset: Use tokenized_paperswithcode dataset.\n",
      "[11/24/2021 13:37:50] INFO - dataset: Preprocessed dataset. Use default vocab_size=83931.\n",
      "[11/24/2021 13:37:50] INFO - dataset: Successfully converted dataset to dataloader.\n"
     ]
    }
   ],
   "source": [
    "data_args = DataArguments()\n",
    "training_args = TrainerArguments()\n",
    "model_args = ModelArguments()\n",
    "\n",
    "data_args.data_dir = \"../data/\"\n",
    "data_args.asset_dir = \"../assets/\"\n",
    "model_args.asset_dir = data_args.asset_dir\n",
    "data_args.seed = training_args.seed\n",
    "data_args.max_seq_len = model_args.max_seq_len\n",
    "\n",
    "data_args.init_pct = 1\n",
    "\n",
    "train_dataset, model_args.vocab_size, model_args.num_labels = build_dataset(data_args, \"train\")\n",
    "train_dataloader = build_dataloader(train_dataset, data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import NaiveTrainer\n",
    "trainer = NaiveTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "loss_fc = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  7., 13., 10.,  7.,  7.,  7., 10.,  2., 15.,  7., 14.,  7., 15.,\n",
       "        10., 10.,  7.,  4., 14., 14.,  8.,  7.,  3., 14., 12., 10.,  8., 12.,\n",
       "        10.,  7., 15.,  7.,  9.,  7., 14.,  3., 12., 15.,  4., 15., 14.,  7.,\n",
       "        10.,  7.,  7.,  7.,  4.,  5.,  4., 12.,  3.,  7., 15.,  3., 14., 12.,\n",
       "        12., 10.,  7., 12.,  3.,  3., 15.,  7.,  8., 15.,  3.,  7.,  2., 14.,\n",
       "         3.,  7.,  3., 10., 12.,  3.,  7., 15., 14., 15., 13.,  7.,  4., 12.,\n",
       "         7.,  7.,  4., 14.,  3., 10.,  8.,  0., 15.,  7.,  3.,  7.,  8.,  7.,\n",
       "        15.,  7., 15., 14., 15., 15.,  4.,  3., 14., 12., 15.,  3.,  1.,  5.,\n",
       "         3.,  3., 10.,  4., 10., 10.,  7.,  6., 10.,  7.,  3.,  5.,  6., 10.,\n",
       "        10., 11.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from toma import toma\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from batchbald_redux import joint_entropy\n",
    "\n",
    "K = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_mixture_prob_dist(p1, p2, m):\n",
    "    return (1.0 - m) * np.asarray(p1) + m * np.asarray(p2)\n",
    "\n",
    "\n",
    "p1 = [0.7, 0.1, 0.1, 0.1]\n",
    "p2 = [0.3, 0.3, 0.2, 0.2]\n",
    "y1_ws = [get_mixture_prob_dist(p1, p2, m) for m in np.linspace(0, 1, K)]\n",
    "\n",
    "p1 = [0.1, 0.7, 0.1, 0.1]\n",
    "p2 = [0.2, 0.3, 0.3, 0.2]\n",
    "y2_ws = [get_mixture_prob_dist(p1, p2, m) for m in np.linspace(0, 1, K)]\n",
    "\n",
    "p1 = [0.1, 0.1, 0.7, 0.1]\n",
    "p2 = [0.2, 0.2, 0.3, 0.3]\n",
    "y3_ws = [get_mixture_prob_dist(p1, p2, m) for m in np.linspace(0, 1, K)]\n",
    "\n",
    "p1 = [0.1, 0.1, 0.1, 0.7]\n",
    "p2 = [0.3, 0.2, 0.2, 0.3]\n",
    "y4_ws = [get_mixture_prob_dist(p1, p2, m) for m in np.linspace(0, 1, K)]\n",
    "\n",
    "\n",
    "def nested_to_tensor(l):\n",
    "    return torch.stack(list(map(torch.as_tensor, l)))\n",
    "\n",
    "\n",
    "ys_ws = nested_to_tensor([y1_ws, y2_ws, y3_ws, y4_ws])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_conditional_entropy(probs_N_K_C: torch.Tensor) -> torch.Tensor:\n",
    "    N, K, C = probs_N_K_C.shape\n",
    "\n",
    "    entropies_N = torch.empty(N, dtype=torch.double)\n",
    "\n",
    "    pbar = tqdm(total=N, desc=\"Conditional Entropy\", leave=False)\n",
    "\n",
    "    @toma.execute.chunked(probs_N_K_C, 1024)\n",
    "    def compute(probs_n_K_C, start: int, end: int):\n",
    "        nats_n_K_C = probs_n_K_C * torch.log(probs_n_K_C)\n",
    "        nats_n_K_C[probs_n_K_C == 0] = 0.0\n",
    "\n",
    "        entropies_N[start:end].copy_(-torch.sum(nats_n_K_C, dim=(1, 2)) / K)\n",
    "        pbar.update(end - start)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    return entropies_N\n",
    "\n",
    "\n",
    "def compute_entropy(probs_N_K_C: torch.Tensor) -> torch.Tensor:\n",
    "    N, K, C = probs_N_K_C.shape\n",
    "\n",
    "    entropies_N = torch.empty(N, dtype=torch.double)\n",
    "\n",
    "    pbar = tqdm(total=N, desc=\"Entropy\", leave=False)\n",
    "\n",
    "    @toma.execute.chunked(probs_N_K_C, 1024)\n",
    "    def compute(probs_n_K_C, start: int, end: int):\n",
    "        mean_probs_n_C = probs_n_K_C.mean(dim=1)\n",
    "        nats_n_C = mean_probs_n_C * torch.log(mean_probs_n_C)\n",
    "        nats_n_C[mean_probs_n_C == 0] = 0.0\n",
    "\n",
    "        entropies_N[start:end].copy_(-torch.sum(nats_n_C, dim=1))\n",
    "        pbar.update(end - start)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    return entropies_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: toma in c:\\users\\pha\\anaconda3\\envs\\bnn\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\pha\\anaconda3\\envs\\bnn\\lib\\site-packages (from toma) (5.8.0)\n",
      "Requirement already satisfied: torch in c:\\users\\pha\\anaconda3\\envs\\bnn\\lib\\site-packages (from toma) (1.8.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\pha\\anaconda3\\envs\\bnn\\lib\\site-packages (from torch->toma) (3.10.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\pha\\anaconda3\\envs\\bnn\\lib\\site-packages (from torch->toma) (1.21.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install toma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a61f66c06004f41873e28d463485a6c1d6ba80ac66cbf9cb782a5e0aa1d62397"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('bnn': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
