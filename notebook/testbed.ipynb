{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code\")\n",
    "from dataset import build_dataset, build_dataloader\n",
    "from config import DataArguments, TrainerArguments, ModelArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11/23/2021 14:31:45] INFO - dataset: Initialize Train Dataset.\n",
      "[11/23/2021 14:31:45] INFO - dataset: Remove abstract.\n",
      "[11/23/2021 14:31:45] INFO - dataset: Remove task_id\n",
      "[11/23/2021 14:31:45] INFO - dataset: Use the full dataset, for train dataset of total 47250 papers.\n",
      "[11/23/2021 14:31:45] INFO - dataset: Train dataset was successfully initialized.\n",
      "[11/23/2021 14:31:47] INFO - preprocess: Successfully loaded Spacy Tokenizer, en_core_web_trf\n",
      "[11/23/2021 14:31:47] INFO - dataset: Successfully loaded mapper file ..\\assets\\area2idx.json\n",
      "[11/23/2021 14:31:52] WARNING - datasets.fingerprint: Parameter 'function'=<function build_dataset.<locals>.batch_encode at 0x00000255285EB678> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|██████████| 48/48 [31:09<00:00, 38.94s/ba]\n",
      "[11/23/2021 15:03:01] INFO - dataset: train dataset was properly preprocessed.\n"
     ]
    }
   ],
   "source": [
    "data_args = DataArguments()\n",
    "training_args = TrainerArguments()\n",
    "model_args = ModelArguments()\n",
    "\n",
    "data_args.data_dir = \"../data/\"\n",
    "data_args.asset_dir = \"../assets/\"\n",
    "model_args.asset_dir = data_args.asset_dir\n",
    "data_args.seed = training_args.seed\n",
    "data_args.max_seq_len = model_args.max_seq_len\n",
    "\n",
    "data_args.init_pct = 1\n",
    "\n",
    "train_dataset, model_args.vocab_size, model_args.num_labels = build_dataset(data_args, \"train\")\n",
    "# train_dataloader = build_dataloader(train_dataset, data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data_dir = \"../data/tokenized_paperswtihcode/\"\n",
    "\n",
    "train_dataset.save_to_disk(f\"{preprocessed_data_dir}/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11/23/2021 15:48:15] INFO - dataset: Initialize Valid Dataset.\n",
      "[11/23/2021 15:48:15] INFO - dataset: Remove abstract.\n",
      "[11/23/2021 15:48:15] INFO - dataset: Remove task_id\n",
      "[11/23/2021 15:48:15] INFO - dataset: Use the full dataset, for valid dataset of total 2625 papers.\n",
      "[11/23/2021 15:48:15] INFO - dataset: Valid dataset was successfully initialized.\n",
      "[11/23/2021 15:48:17] INFO - preprocess: Successfully loaded Spacy Tokenizer, en_core_web_trf\n",
      "[11/23/2021 15:48:17] INFO - dataset: Successfully loaded mapper file ..\\assets\\area2idx.json\n",
      "100%|██████████| 3/3 [01:45<00:00, 35.02s/ba]\n",
      "[11/23/2021 15:50:05] INFO - dataset: valid dataset was properly preprocessed.\n",
      "[11/23/2021 15:50:05] INFO - dataset: Initialize Test Dataset.\n",
      "[11/23/2021 15:50:05] INFO - dataset: Remove abstract.\n",
      "[11/23/2021 15:50:05] INFO - dataset: Remove task_id\n",
      "[11/23/2021 15:50:05] INFO - dataset: Use the full dataset, for test dataset of total 2625 papers.\n",
      "[11/23/2021 15:50:05] INFO - dataset: Test dataset was successfully initialized.\n",
      "[11/23/2021 15:50:07] INFO - preprocess: Successfully loaded Spacy Tokenizer, en_core_web_trf\n",
      "[11/23/2021 15:50:07] INFO - dataset: Successfully loaded mapper file ..\\assets\\area2idx.json\n",
      "100%|██████████| 3/3 [01:46<00:00, 35.38s/ba]\n",
      "[11/23/2021 15:51:56] INFO - dataset: test dataset was properly preprocessed.\n"
     ]
    }
   ],
   "source": [
    "valid_dataset, model_args.vocab_size, model_args.num_labels = build_dataset(data_args, \"valid\")\n",
    "test_dataset, model_args.vocab_size, model_args.num_labels = build_dataset(data_args, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset.save_to_disk(f'{preprocessed_data_dir}/valid')\n",
    "test_dataset.save_to_disk(f'{preprocessed_data_dir}/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "confidence_level = np.random.randint(low=0, high=1, size=(2625,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_confidence_level(x, new_info_x):\n",
    "\n",
    "    x[\"confidence_level\"] = new_info_x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11/21/2021 15:34:25] INFO - trainer: Bert was selected. Start Transformer setup.\n",
      "[11/21/2021 15:34:25] INFO - trainer: Successfully setup transformer settings.\n"
     ]
    }
   ],
   "source": [
    "from trainer import NaiveTrainer\n",
    "\n",
    "training_args.do_train = True\n",
    "\n",
    "trainer = NaiveTrainer(\n",
    "    training_args,\n",
    "    model_args,\n",
    "    training_dataset=train_dataloader if training_args.do_train else None,\n",
    "    validation_dataset=valid_dataloader if training_args.do_valid else None,\n",
    "    test_dataset=test_dataloader if training_args.do_test else None,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a61f66c06004f41873e28d463485a6c1d6ba80ac66cbf9cb782a5e0aa1d62397"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('bnn': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
