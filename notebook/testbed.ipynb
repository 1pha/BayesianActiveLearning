{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pha\\anaconda3\\envs\\bnn\\lib\\site-packages\\torch\\cuda\\__init__.py:80: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code\")\n",
    "from dataset import build_dataset, build_dataloader\n",
    "from config import DataArguments, TrainerArguments, ModelArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11/20/2021 21:25:25] INFO - dataset: Initialize Train Dataset.\n",
      "[11/20/2021 21:25:25] INFO - dataset: Remove abstract.\n",
      "[11/20/2021 21:25:25] INFO - dataset: Remove task_id\n",
      "[11/20/2021 21:25:25] INFO - dataset: Use 0.05% of the total dataset.\n",
      "[11/20/2021 21:25:26] INFO - dataset: Total 2363 of papers will be used.\n",
      "[11/20/2021 21:25:26] INFO - dataset: Train dataset was successfully initialized.\n",
      "[11/20/2021 21:25:29] INFO - preprocess: Successfully loaded Spacy Tokenizer, en_core_web_trf\n",
      "[11/20/2021 21:25:29] INFO - dataset: Successfully loaded mapper file ..\\assets\\area2idx.json\n",
      "[11/20/2021 21:25:31] WARNING - datasets.fingerprint: Parameter 'function'=<function build_dataset.<locals>.batch_encode at 0x000001EAA610D558> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "  0%|          | 0/3 [00:00<?, ?ba/s]C:\\Users\\pha\\anaconda3\\envs\\bnn\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "100%|██████████| 3/3 [01:32<00:00, 30.68s/ba]\n",
      "[11/20/2021 21:27:03] INFO - dataset: Successfully converted dataset to dataloader.\n"
     ]
    }
   ],
   "source": [
    "data_args = DataArguments\n",
    "training_args = TrainerArguments\n",
    "model_args = ModelArguments\n",
    "\n",
    "data_args.data_dir = \"../data/\"\n",
    "data_args.seed = training_args.seed\n",
    "data_args.max_seq_len = model_args.max_seq_len\n",
    "\n",
    "dataset, model_args.vocab_size, model_args.num_labels = build_dataset(data_args, \"train\")\n",
    "dataloader = build_dataloader(dataset, data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_model import load_model\n",
    "\n",
    "# model_args.vocab_size = 83931\n",
    "# model_args.model_name_or_path = 'bert'\n",
    "# model_args.num_labels = 16\n",
    "model = load_model(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17120/1205699559.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\bnn\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pha\\codespace\\bayesian_final_project\\code\\models\\transformer_models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_ids\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"label\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"logits\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'label'"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dataloader))\n",
    "logits, predicted_class = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 35166,  3480,  ...,     0,     0,     0],\n",
       "         [    0,   100,    12,  ...,     0,     0,     0],\n",
       "         [    0, 24514, 21464,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [    0, 43780,   154,  ...,     0,     0,     0],\n",
       "         [    0, 14563,  7367,  ...,     0,     0,     0],\n",
       "         [    0, 35166,  2239,  ...,     0,     0,     0]]),\n",
       " 'label': tensor([ 7, 10,  7, 14, 13, 12, 14, 14,  7,  3, 10,  8, 10, 12, 14,  7,  6, 10,\n",
       "          6,  7, 10,  3, 15, 15, 10, 10,  8,  7, 12, 15, 10, 14,  4,  7, 15,  3,\n",
       "          3, 15, 10,  7, 14, 13, 15, 10, 12,  3,  7, 12, 13,  7,  5, 11, 12, 15,\n",
       "          7,  7, 12,  4,  7,  7,  8,  3,  7,  5, 12, 15, 13,  3,  7,  4, 15, 12,\n",
       "         12, 12, 14,  5, 11, 10,  4,  7, 13,  9,  4,  7,  7, 12, 11,  7, 10, 14,\n",
       "         15,  3, 10, 15,  7,  4,  4,  8, 14,  7, 12,  5, 13,  7,  3,  7,  2, 14,\n",
       "          4,  7,  7, 11, 15, 15,  6, 15, 14,  3,  7, 15, 15,  3, 10,  7, 12,  3,\n",
       "          3,  6])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.model(**batch)\n",
    "loss = out[\"loss\"]\n",
    "logits = out[\"logits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.8167, grad_fn=<NllLossBackward0>),\n",
       " tensor([[ 0.3428,  0.2857, -0.2186,  ..., -0.3939,  0.0539, -0.1727],\n",
       "         [ 0.1125,  0.5036, -0.0297,  ..., -0.1842,  0.2904,  0.1124],\n",
       "         [ 0.2734,  0.4260,  0.0040,  ..., -0.2952,  0.1796,  0.1000],\n",
       "         ...,\n",
       "         [ 0.0574,  0.4719, -0.1994,  ..., -0.0546,  0.2691, -0.0266],\n",
       "         [ 0.1133,  0.2223, -0.0434,  ..., -0.1677,  0.2933,  0.1312],\n",
       "         [ 0.3418,  0.6447, -0.1163,  ...,  0.1065,  0.1086, -0.0074]],\n",
       "        grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8167, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(logits, batch[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a61f66c06004f41873e28d463485a6c1d6ba80ac66cbf9cb782a5e0aa1d62397"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('bnn': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
